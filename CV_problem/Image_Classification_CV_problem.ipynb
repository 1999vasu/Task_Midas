{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eg1PtMCJPNHw"
   },
   "source": [
    "# Mounting google drive in colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rGA4_HT6PNHy"
   },
   "source": [
    "1. Using google.colab library for mounting drive\n",
    "2. This is necessary for loading all the test and train pickle files provided(alreday stored in my drive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "eCKebiIL98dH",
    "outputId": "28bed702-5ebe-4263-bdf8-cae864b05afb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /colab; to attempt to forcibly remount, call drive.mount(\"/colab\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/colab')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X9-WKtoOPNH4"
   },
   "source": [
    "Importing all the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "87DhkPqi9vnb",
    "outputId": "aaeb2bc0-3ac6-4ab2-fa9c-53cb0b4597b0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers import Reshape, Activation, Convolution2D, Input, MaxPooling2D, BatchNormalization, Flatten, Dense, Dropout, Conv2D,MaxPool2D, ZeroPadding2D\n",
    "from keras.models import Sequential\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n87Luva9PNH-"
   },
   "source": [
    "# Image preprocessing for trainnig and separation of validation data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-4bCvAzNPNH_"
   },
   "source": [
    "1. The following code cell first loads all the respective pickle files in respective variables, and than converted to numpy nd array and reshaped the image data to shape (-1,28,28,1). Furthter we created one hot encoding of all classes using sklearn library.\n",
    "2. This was need to be done so that we can feed our training data to keras model. One hot encoding was created because we need to do multiclass claassification and we need to give each class an equal weight. \n",
    "\n",
    "\n",
    "Note : No need to divide image data with 255.0 as later our BatchNormalization already do this after first convolution layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "EeObXW849vng",
    "outputId": "d869fbda-2b32-4919-8d48-a3edda25de2f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000\n"
     ]
    }
   ],
   "source": [
    "# Loading training images\n",
    "train_data = []\n",
    "with open('/colab/My Drive/Vision_task_dataset_public/train_image.pkl','rb') as handle:\n",
    "    train_data = pickle.load(handle)\n",
    "\n",
    "# Loading  training labels   \n",
    "target = []\n",
    "with open('/colab/My Drive/Vision_task_dataset_public/train_label.pkl','rb') as handle:\n",
    "    target = pickle.load(handle)\n",
    "print(len(target))\n",
    "\n",
    "# Converting image data to numpy nd array\n",
    "train_data = np.array(train_data,dtype = np.float32)\n",
    "\n",
    "# Reshaping images to shape(-1,28,28,1) since we need to feed this to our keras model\n",
    "train_data = train_data.reshape(-1,28,28,1)\n",
    "\n",
    "# Converting image label data to numpy nd array\n",
    "target = np.array(target)\n",
    "\n",
    "# Reshaping labels to sahpe(-1,1) so that we can feed this to sklearn OneHotEncoder which taked 2D array\n",
    "target = target.reshape(-1,1)\n",
    "\n",
    "\n",
    "# Importing sklearn libraray for creating One Hot Encoding and than storing it in target variable\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder(handle_unknown='ignore',sparse = False)\n",
    "target = enc.fit_transform(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "xZGPfEH_6S9O",
    "outputId": "04ce3cbc-3731-4b9b-b439-595c272c946d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 2 3 6]\n"
     ]
    }
   ],
   "source": [
    "# Printing classes and storing them in classes variable for future use. (While converting back model predictions \n",
    "# to the previous classes)\n",
    "classes = enc.categories_\n",
    "classes = classes[0]\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "HnvCeNsAQyWJ",
    "outputId": "e83dcfe5-41a5-4d3c-a71a-ebd6a86ae613"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8000, 4) (8000, 28, 28, 1)\n",
      "(7200, 28, 28, 1) (800, 28, 28, 1) (7200, 4) (800, 4)\n"
     ]
    }
   ],
   "source": [
    "print(target.shape,train_data.shape)\n",
    "\n",
    "\n",
    "# Importing library required for dividing training and validation data and than spliting given in 0.9 training \n",
    "# data and 0.1 validation data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_data, target, test_size=0.10, random_state=42)\n",
    "\n",
    "# Printing their shapes just to ensure correctness\n",
    "print(X_train.shape,X_test.shape,y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_kBZTWepPNIK"
   },
   "source": [
    "# Data Augmentation on training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oov5H11KPNIL"
   },
   "source": [
    "1. The following code cell performs data augmentation by converting one training image to total of 8 images, \n",
    "namely, same, rotated three times 90 degree, and another 4 by fliping all of them vertically\n",
    "2. This was necessary since we have very small training data, we need to somehow increase this so thatour model \n",
    "learns better features, and to improve testing accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "u51R1TslxPcF",
    "outputId": "b609a596-3dc2-43ec-a0af-de9247b08576"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7200/7200 [00:00<00:00, 26583.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57600\n",
      "57600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Importing tqdm library just to visualize progress\n",
    "from tqdm import tqdm\n",
    "\n",
    "train_data = X_train\n",
    "target = y_train\n",
    "\n",
    "# Initializing empty list to story all datas\n",
    "train = []\n",
    "tar = []\n",
    "\n",
    "# Looping over every train image\n",
    "for ix in tqdm(range(len(train_data))):\n",
    "    temp = train_data[ix]\n",
    "  \n",
    "    # Storing normal image and it vertical flipped version     \n",
    "    train.append(temp)\n",
    "    train.append(np.flip(temp,axis=1))\n",
    "  \n",
    "    # Rotating image by 90 degree     \n",
    "    rot = np.rot90(temp,1,(0,1))\n",
    "    \n",
    "    # Storing the next rotated image and its flipped version\n",
    "    train.append(rot)\n",
    "    train.append(np.flip(rot,axis=1))\n",
    "     \n",
    "    # Rotating image by 90 degree \n",
    "    rot = np.rot90(rot,1,(0,1))\n",
    "    \n",
    "    # Storing the next rotated image and its flipped version\n",
    "    train.append(rot)\n",
    "    train.append(np.flip(rot,axis=1))\n",
    "  \n",
    "    # Rotating image by 90 degree \n",
    "    rot = np.rot90(rot,1,(0,1))\n",
    "    # Storing the next rotated image and its flipped version\n",
    "    train.append(rot)\n",
    "    train.append(np.flip(rot,axis=1))\n",
    "  \n",
    "    # Storing image labels for complete augmented data\n",
    "    tar.append(target[ix])\n",
    "    tar.append(target[ix])\n",
    "    tar.append(target[ix])\n",
    "    tar.append(target[ix])\n",
    "    tar.append(target[ix])\n",
    "    tar.append(target[ix])\n",
    "    tar.append(target[ix])\n",
    "    tar.append(target[ix])\n",
    "\n",
    "# Printing lengths of list just for checking correctess\n",
    "print(len(train))\n",
    "print(len(tar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "g3-kaF1mSz0Z",
    "outputId": "4861e262-acc0-4c8e-f683-647d36ff79d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(57600, 28, 28, 1) (57600, 4)\n"
     ]
    }
   ],
   "source": [
    "# Converting them to numpy nd array\n",
    "X_train = np.array(train)\n",
    "y_train = np.array(tar)\n",
    "\n",
    "# Printing their shapes just to verify the correctness\n",
    "print(X_train.shape,y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HeGU3f_oPNIT"
   },
   "source": [
    "# Designing our Keras Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L8g-usjhPNIU"
   },
   "source": [
    "1. The following code cell lays down the structure of our keras model.\n",
    "2. Our model is an example of Convolution Neural Network. I used it because it can learn image features very \n",
    "accurately and can give better accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J1Eq2CQzPNIV"
   },
   "source": [
    "# General Approach behind Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_zWbhRoAPNIV"
   },
   "source": [
    "We have used specific layers to perform specific tasks\n",
    "1. Convolution2D : This layer is the base of The CNN model. It creates the kernel with given size. Our model learns various features of training data\n",
    "2. LeakyReLU: It is advanced activation layer for implementation of LeakyReLU\n",
    "3. BatchNormalization: This is the layer which normalized data from previous layer, and this results in 10 times faster learning of our model\n",
    "4. MaxPool2D: This layer helps to reduce the size by taking maximum value in the size of kernel provided and than reduces that kernel size to a single value. \n",
    "5. Flatten: This layer helps to flatten the output of previous layer to a simple 2D array\n",
    "6. Dense: This layer acts like a neural network layer.\n",
    "\n",
    "Activations Used:\n",
    "1. LeakyReLU: This activation is far much better than ReLu as it provides learning when value becomes negative also.\n",
    "2. Relu: This activation is also much better than other activations like tanh, sigmoid etc as it never faces problem of gradient exploding and gradient vanishing.\n",
    "3. Softmax: For multiclass classification we have to use this activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 748
    },
    "colab_type": "code",
    "id": "pc8UjgWL_iq4",
    "outputId": "30a5ca21-df56-4683-c976-f50c1f7954fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_4 (Conv2D)            (None, 28, 28, 64)        640       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 28, 28, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 14, 14, 512)       295424    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 7, 7, 1024)        4719616   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 7, 7, 1024)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 7, 7, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 3, 3, 1024)        0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1024)              9438208   \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 4)                 2052      \n",
      "=================================================================\n",
      "Total params: 14,993,284\n",
      "Trainable params: 14,987,012\n",
      "Non-trainable params: 6,272\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Convolution2D(64, (3,3), padding='same', input_shape=(28,28,1)))\n",
    "model.add(LeakyReLU(alpha = 0.1))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Convolution2D(512, (3,3), padding='same'))\n",
    "model.add(LeakyReLU(alpha = 0.1))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Convolution2D(1024, (3,3), padding='same'))\n",
    "model.add(LeakyReLU(alpha = 0.1))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1024,activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(512,activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(4,activation = 'softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HWNLLhS4PNIY"
   },
   "source": [
    "# Training our model and saving model weights progressively"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3jPJS7MWPNIZ"
   },
   "source": [
    "1. The following code cell first setups the environment for training and than trains the model\n",
    "2. We need to setup environment so that we can save the progressive states which are better than previous so helps to save time of re training if anything wrong goes. I keep on increasing batch size becaue it is a better approach, where instead of decreasing learning rate we used to prefer increase the batch size( A recent topic in research)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9TGgsFabcip1"
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "filepath=\"/colab/My Drive/model_weights/zweights-{val_acc:.4f}.h5\"\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=True, mode='max', period=1)\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hWqVz0t_AUXq"
   },
   "outputs": [],
   "source": [
    "# Compiling our model\n",
    "model.compile(optimizer = 'adam',loss = 'categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 819
    },
    "colab_type": "code",
    "id": "Bo5aTBmvBGvC",
    "outputId": "a463c6bd-3f10-48de-dbe1-2781cf3bbf5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(57600, 28, 28, 1) (800, 28, 28, 1) (57600, 4) (800, 4)\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 57600 samples, validate on 800 samples\n",
      "Epoch 1/10\n",
      "57600/57600 [==============================] - 72s 1ms/step - loss: 0.5112 - acc: 0.8041 - val_loss: 0.4513 - val_acc: 0.8263\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.82625, saving model to /colab/My Drive/model_weights/zweights-0.8263.h5\n",
      "Epoch 2/10\n",
      "57600/57600 [==============================] - 69s 1ms/step - loss: 0.3429 - acc: 0.8683 - val_loss: 0.4397 - val_acc: 0.8275\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.82625 to 0.82750, saving model to /colab/My Drive/model_weights/zweights-0.8275.h5\n",
      "Epoch 3/10\n",
      "57600/57600 [==============================] - 69s 1ms/step - loss: 0.2864 - acc: 0.8901 - val_loss: 0.3806 - val_acc: 0.8438\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.82750 to 0.84375, saving model to /colab/My Drive/model_weights/zweights-0.8438.h5\n",
      "Epoch 4/10\n",
      "57600/57600 [==============================] - 69s 1ms/step - loss: 0.2348 - acc: 0.9074 - val_loss: 0.4780 - val_acc: 0.8475\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.84375 to 0.84750, saving model to /colab/My Drive/model_weights/zweights-0.8475.h5\n",
      "Epoch 5/10\n",
      "57600/57600 [==============================] - 70s 1ms/step - loss: 0.1884 - acc: 0.9270 - val_loss: 0.4639 - val_acc: 0.8512\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.84750 to 0.85125, saving model to /colab/My Drive/model_weights/zweights-0.8512.h5\n",
      "Epoch 6/10\n",
      "57600/57600 [==============================] - 69s 1ms/step - loss: 0.1510 - acc: 0.9411 - val_loss: 0.4679 - val_acc: 0.8712\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.85125 to 0.87125, saving model to /colab/My Drive/model_weights/zweights-0.8712.h5\n",
      "Epoch 7/10\n",
      "57600/57600 [==============================] - 69s 1ms/step - loss: 0.1203 - acc: 0.9538 - val_loss: 0.4879 - val_acc: 0.8662\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.87125\n",
      "Epoch 8/10\n",
      "57600/57600 [==============================] - 69s 1ms/step - loss: 0.1089 - acc: 0.9587 - val_loss: 0.5617 - val_acc: 0.8575\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.87125\n",
      "Epoch 9/10\n",
      "57600/57600 [==============================] - 69s 1ms/step - loss: 0.0824 - acc: 0.9691 - val_loss: 0.4802 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.87125 to 0.87625, saving model to /colab/My Drive/model_weights/zweights-0.8762.h5\n",
      "Epoch 10/10\n",
      "57600/57600 [==============================] - 69s 1ms/step - loss: 0.0713 - acc: 0.9731 - val_loss: 0.6225 - val_acc: 0.8638\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.87625\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe046efc320>"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(X_train.shape,X_test.shape,y_train.shape, y_test.shape)\n",
    "model.fit(X_train,y_train, batch_size = 64, epochs=10,callbacks = callbacks_list, validation_data = [X_test, y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 748
    },
    "colab_type": "code",
    "id": "SI1aJCfEgkuL",
    "outputId": "f8e5f8a1-b125-451a-c5f4-49f140f00d7c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(57600, 28, 28, 1) (800, 28, 28, 1) (57600, 4) (800, 4)\n",
      "Train on 57600 samples, validate on 800 samples\n",
      "Epoch 1/10\n",
      "57600/57600 [==============================] - 53s 917us/step - loss: 0.0196 - acc: 0.9930 - val_loss: 0.6532 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00001: val_acc did not improve from 0.87625\n",
      "Epoch 2/10\n",
      "57600/57600 [==============================] - 52s 898us/step - loss: 0.0090 - acc: 0.9970 - val_loss: 0.6243 - val_acc: 0.8838\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.87625 to 0.88375, saving model to /colab/My Drive/model_weights/zweights-0.8838.h5\n",
      "Epoch 3/10\n",
      "57600/57600 [==============================] - 52s 898us/step - loss: 0.0149 - acc: 0.9947 - val_loss: 0.6894 - val_acc: 0.8662\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.88375\n",
      "Epoch 4/10\n",
      "57600/57600 [==============================] - 51s 892us/step - loss: 0.0266 - acc: 0.9902 - val_loss: 0.7239 - val_acc: 0.8612\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.88375\n",
      "Epoch 5/10\n",
      "57600/57600 [==============================] - 51s 891us/step - loss: 0.0220 - acc: 0.9923 - val_loss: 0.8627 - val_acc: 0.8562\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.88375\n",
      "Epoch 6/10\n",
      "57600/57600 [==============================] - 51s 891us/step - loss: 0.0167 - acc: 0.9945 - val_loss: 0.9917 - val_acc: 0.8387\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.88375\n",
      "Epoch 7/10\n",
      "57600/57600 [==============================] - 51s 892us/step - loss: 0.0227 - acc: 0.9919 - val_loss: 0.8238 - val_acc: 0.8425\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.88375\n",
      "Epoch 8/10\n",
      "57600/57600 [==============================] - 51s 893us/step - loss: 0.0239 - acc: 0.9918 - val_loss: 0.7738 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.88375\n",
      "Epoch 9/10\n",
      "57600/57600 [==============================] - 51s 891us/step - loss: 0.0152 - acc: 0.9947 - val_loss: 0.8689 - val_acc: 0.8612\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.88375\n",
      "Epoch 10/10\n",
      "57600/57600 [==============================] - 51s 892us/step - loss: 0.0160 - acc: 0.9945 - val_loss: 0.8591 - val_acc: 0.8650\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.88375\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe046efc198>"
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(X_train.shape,X_test.shape,y_train.shape, y_test.shape)\n",
    "model.fit(X_train,y_train, batch_size = 128, epochs=10,callbacks = callbacks_list, validation_data = [X_test, y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 748
    },
    "colab_type": "code",
    "id": "y5Hdjr5kkuRD",
    "outputId": "2f938f25-701e-478d-9e78-901610a4dd4d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(57600, 28, 28, 1) (800, 28, 28, 1) (57600, 4) (800, 4)\n",
      "Train on 57600 samples, validate on 800 samples\n",
      "Epoch 1/10\n",
      "57600/57600 [==============================] - 51s 893us/step - loss: 0.0152 - acc: 0.9945 - val_loss: 0.8637 - val_acc: 0.8638\n",
      "\n",
      "Epoch 00001: val_acc did not improve from 0.88375\n",
      "Epoch 2/10\n",
      "57600/57600 [==============================] - 51s 893us/step - loss: 0.0202 - acc: 0.9936 - val_loss: 0.8540 - val_acc: 0.8712\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.88375\n",
      "Epoch 3/10\n",
      "57600/57600 [==============================] - 51s 892us/step - loss: 0.0195 - acc: 0.9936 - val_loss: 0.7841 - val_acc: 0.8725\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.88375\n",
      "Epoch 4/10\n",
      "57600/57600 [==============================] - 51s 892us/step - loss: 0.0119 - acc: 0.9960 - val_loss: 1.0024 - val_acc: 0.8562\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.88375\n",
      "Epoch 5/10\n",
      "57600/57600 [==============================] - 51s 893us/step - loss: 0.0124 - acc: 0.9956 - val_loss: 0.8779 - val_acc: 0.8662\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.88375\n",
      "Epoch 6/10\n",
      "57600/57600 [==============================] - 51s 893us/step - loss: 0.0115 - acc: 0.9964 - val_loss: 0.9564 - val_acc: 0.8650\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.88375\n",
      "Epoch 7/10\n",
      "57600/57600 [==============================] - 51s 893us/step - loss: 0.0092 - acc: 0.9970 - val_loss: 1.0455 - val_acc: 0.8588\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.88375\n",
      "Epoch 8/10\n",
      "57600/57600 [==============================] - 52s 905us/step - loss: 0.0122 - acc: 0.9962 - val_loss: 0.9216 - val_acc: 0.8650\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.88375\n",
      "Epoch 9/10\n",
      "57600/57600 [==============================] - 52s 904us/step - loss: 0.0137 - acc: 0.9952 - val_loss: 0.8474 - val_acc: 0.8662\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.88375\n",
      "Epoch 10/10\n",
      "57600/57600 [==============================] - 52s 903us/step - loss: 0.0143 - acc: 0.9951 - val_loss: 1.0624 - val_acc: 0.8650\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.88375\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe046259c18>"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(X_train.shape,X_test.shape,y_train.shape, y_test.shape)\n",
    "model.fit(X_train,y_train, batch_size = 128, epochs=10,callbacks = callbacks_list, validation_data = [X_test, y_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "upAT_jqdPNIu"
   },
   "source": [
    "# Loading best model weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_fj8BfRhPNIv"
   },
   "source": [
    "1. The following code cels loads best model weights from the stored weights files\n",
    "2. This is necessary because it is quite possible that our model overfits the data, so we need to do early stopping by loading the best weights file with maximum val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "4frpx6UkAHK2",
    "outputId": "bf1c9bb1-debc-497d-c40a-a8174db27671"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zweights-0.8838.h5\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "filename=\"/colab/My Drive/model_weights/\"\n",
    "bestfile=\"zweights-0.0000.h5\"\n",
    "for file in os.listdir(filename):\n",
    "    if(file.startswith(\"zweights-0\")):\n",
    "        if(bestfile<file):\n",
    "            bestfile=file\n",
    "print(bestfile)\n",
    "model.load_weights(filename+bestfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fAP8YjnfPNIz"
   },
   "source": [
    "# Loading test_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "By5eHfik5hBR",
    "outputId": "67e81f22-d5a8-43b3-9f2e-ad904a9f1aa6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "test_data = []\n",
    "with open('/colab/My Drive/Vision_task_dataset_public/test_image.pkl','rb') as handle:\n",
    "    test_data = pickle.load(handle)\n",
    "print(type(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "yd-vTKj_6DSm",
    "outputId": "d8523aa7-e269-4dc5-c5b4-2f99e4b5eeb4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n"
     ]
    }
   ],
   "source": [
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hn57t7yjPNI6"
   },
   "source": [
    "# Making final Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "JvqiWopH6H-S",
    "outputId": "d5942a41-cd64-4a2c-99a3-263193fd7d96"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 4)\n",
      "(2000,)\n",
      "[[0.000e+00 0.000e+00]\n",
      " [1.000e+00 0.000e+00]\n",
      " [2.000e+00 0.000e+00]\n",
      " ...\n",
      " [1.997e+03 6.000e+00]\n",
      " [1.998e+03 6.000e+00]\n",
      " [1.999e+03 6.000e+00]]\n"
     ]
    }
   ],
   "source": [
    "# Converting to numpy nd array\n",
    "test_data = np.array(test_data)\n",
    "\n",
    "# Reshapint test_data to shape of(-1,28,28,1) so that we can feed it to our model\n",
    "test_data = test_data.reshape(-1,28,28,1)\n",
    "\n",
    "# Making predictions\n",
    "predictions = model.predict(test_data)\n",
    "print(predictions.shape)\n",
    "\n",
    "# Taking argmax of predictions as we got 4 output values against each test_data, which corresponds to each class \n",
    "# probability\n",
    "maxima = np.argmax(predictions,axis=1)\n",
    "print(maxima.shape)\n",
    "\n",
    "\n",
    "# We have to make required csv with two columns one for index other for predicted class\n",
    "final_pred = np.zeros((maxima.shape[0],2))\n",
    "for ix in range(maxima.shape[0]):\n",
    "    final_pred[ix][0] = int(ix)\n",
    "    final_pred[ix][1] = int(classes[maxima[ix]])\n",
    "print(final_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "bSkKABKQQyqG",
    "outputId": "6a160553-f5cd-4d06-d65d-94dda84eee7b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0., 2., 3., 6.]), array([507, 498, 497, 498]))\n"
     ]
    }
   ],
   "source": [
    "# Just to get intuition if predictions seems somehow valid\n",
    "print(np.unique(final_pred[:,1],return_counts=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DuXi1-4TPNI-"
   },
   "source": [
    "# Storing csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "arwsuw9B6LFz"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(final_pred).to_csv(\"/colab/My Drive/Vasu_Gupta.csv\",header=['image_index','class'],index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rsQpR2AN8tc5"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "preprocessing.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
