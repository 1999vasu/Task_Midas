{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up the environment to start our task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. This code block is importing all libraries required for out task ans taking values of all api keys\n",
    "2. We can't proceed without these libraries, so this is the must step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import json library\n",
    "import simplejson as json\n",
    "\n",
    "# Import the tweepy library, helper library for twitter api\n",
    "import tweepy\n",
    "\n",
    "# Import python file containing my credentials\n",
    "import my_keys_credentials\n",
    "\n",
    "# Import pandas library to display data in tabular format\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing values of all credentials from my_keys_credentials file\n",
    "# Variable names themselves signify their work\n",
    "consumer_key =  my_keys_credentials.CONSUMER_KEY\n",
    "consumer_secret = my_keys_credentials.CONSUMER_SECRET\n",
    "access_key = my_keys_credentials.ACCESS_TOKEN\n",
    "access_secret = my_keys_credentials.ACCESS_SECRET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetching all tweets and dumping them in json file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. This code cell uses the Twitter api to fetch tweets from the specific user's timeline and store them in a file\n",
    "2. Since we need to fetch tweets we had to use twitter api, and than store it in jsonlines file which are easy to extend further as in comparison to json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recieve_tweets(user_name,file_name): \n",
    "    \"\"\"Fucntion which takes user_name, file_name, user_name is the twitter handle of the person from whose timeline \n",
    "    tweets will be take and file_name is the json file name where tweets will be stored\"\"\"          \n",
    "        \n",
    "    # Authorization to for twitter api\n",
    "    auth = tweepy.OAuthHandler(consumer_key, consumer_secret) \n",
    "  \n",
    "    # Access usnig uset access credentials\n",
    "    auth.set_access_token(access_key, access_secret) \n",
    "  \n",
    "    # Calling API function\n",
    "    api = tweepy.API(auth) \n",
    "        \n",
    "    # Maximum Number of tweeets we want to extract\n",
    "    number_of_tweets=10000\n",
    "        \n",
    "    # Extracting required tweets        \n",
    "    tweets = api.user_timeline(screen_name=user_name,count =number_of_tweets)\n",
    "        \n",
    "    # Loop to iterate over every tweet and add it to the json file     \n",
    "    for tweet in tweets:\n",
    "        # Writing into the json target file   \n",
    "        with open(file_name, 'a', encoding='utf8') as file:\n",
    "            file.write(json.dumps(tweet._json))   \n",
    "            file.write('\\n')\n",
    "        \n",
    "# Calling the above function\n",
    "recieve_tweets(\"@midasIIITD\",'tweets.json')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing JSON  files to display the following information for every tweet in a tabular format.\n",
    "\n",
    "● The text of the tweet.\n",
    "\n",
    "● Date and time of the tweet.\n",
    "\n",
    "● The number of favorites/likes.\n",
    "\n",
    "● The number of retweets.\n",
    "\n",
    "● Number of Images present in Tweet. If no image returns None."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The following code cell defines the function which take the file_name and than returns the pandas dataframe to display data in tabular form\n",
    "2. To display the data in tabular format we first need to create the dataframe object containing all rows and all columns data which can than be easily used to display them in quite better form of tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_tweets(tweets_file_name):\n",
    "    \"\"\"Function which take tweets_file_name and returns pandas dataframe of all tweets.\n",
    "    tweets_file_name is the attribute that take address of json file\n",
    "    from where tweets will be take\"\"\"\n",
    "\n",
    "    # Opening json file     \n",
    "    tweets_file = open(tweets_file_name, \"r\",encoding='utf8')\n",
    "    tweets_file.seek(0, 0)\n",
    "    \n",
    "    # Tweets data for all tweets     \n",
    "    tweets_data = []\n",
    "    \n",
    "    # Iterating over all lines in tweets_filename   \n",
    "    for line in tweets_file.readlines():\n",
    "        try:\n",
    "            # Read in one line of the file, convert it into a json object \n",
    "            tweet = json.loads(line)\n",
    "            \n",
    "            temp = []\n",
    "            if 'text' in tweet: # only messages contains 'text' field is a tweet\n",
    "                temp.append(tweet['text'])\n",
    "                temp.append(tweet['created_at'])\n",
    "                temp.append(tweet['favorite_count'])\n",
    "                temp.append(tweet['retweet_count'])\n",
    "                img_count = 0\n",
    "                try:\n",
    "                    for media in tweet['entities']['media']:\n",
    "                        if media['type'] == 'photo':\n",
    "                            img_count += 1\n",
    "                except:\n",
    "                    pass\n",
    "                if(img_count==0):\n",
    "                    img_count = None\n",
    "                temp.append(img_count)\n",
    "                tweets_data.append(temp)\n",
    "\n",
    "        except:\n",
    "            # read in a line is not in JSON format (sometimes error occured)\n",
    "            continue\n",
    "            \n",
    "    # Closing file Handle                   \n",
    "    tweets_file.close()\n",
    "    \n",
    "    # Converting into pandas dataframe\n",
    "    df = pd.DataFrame(tweets_data)\n",
    "    df.columns = [\"Text\", \"Date and Time\", \"Favorite Count\", \"Retweet Count\",\"Images Count\"]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Date and Time</th>\n",
       "      <th>Favorite Count</th>\n",
       "      <th>Retweet Count</th>\n",
       "      <th>Images Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Many Congratulations to @midasIIITD student, S...</td>\n",
       "      <td>Mon Apr 08 07:08:12 +0000 2019</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@midasIIITD thanks all students who have appea...</td>\n",
       "      <td>Mon Apr 08 03:27:42 +0000 2019</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@himanchalchandr Meanwhile, complete CV/NLP ta...</td>\n",
       "      <td>Sun Apr 07 14:17:29 +0000 2019</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@sayangdipto123 Submit as per the guideline ag...</td>\n",
       "      <td>Sun Apr 07 14:17:09 +0000 2019</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>We request all students whose interview are sc...</td>\n",
       "      <td>Sun Apr 07 11:43:24 +0000 2019</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Other queries: \"none of the Tweeter Apis give ...</td>\n",
       "      <td>Sun Apr 07 06:55:19 +0000 2019</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Other queries: \"do we have to make two differe...</td>\n",
       "      <td>Sun Apr 07 06:53:38 +0000 2019</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Other queries: \"If using Twitter api, it does ...</td>\n",
       "      <td>Sun Apr 07 05:32:27 +0000 2019</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Response to some queries asked by students on ...</td>\n",
       "      <td>Sun Apr 07 05:29:40 +0000 2019</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>RT @kdnuggets: Top 8 #Free Must-Read #Books on...</td>\n",
       "      <td>Sat Apr 06 17:11:29 +0000 2019</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>@nupur_baghel @PennDATS Congratulation @nupur_...</td>\n",
       "      <td>Sat Apr 06 16:43:27 +0000 2019</td>\n",
       "      <td>18</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>We have emailed the task details to all candid...</td>\n",
       "      <td>Fri Apr 05 16:08:37 +0000 2019</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>RT @rfpvjr: Our NAACL paper on polarization in...</td>\n",
       "      <td>Fri Apr 05 04:05:11 +0000 2019</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>RT @kdnuggets: Effective Transfer Learning For...</td>\n",
       "      <td>Fri Apr 05 04:04:43 +0000 2019</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>RT @stanfordnlp: What’s new in @Stanford CS224...</td>\n",
       "      <td>Wed Apr 03 18:31:53 +0000 2019</td>\n",
       "      <td>0</td>\n",
       "      <td>59</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>RT @DeepMindAI: Today we're releasing a large-...</td>\n",
       "      <td>Wed Apr 03 17:04:32 +0000 2019</td>\n",
       "      <td>0</td>\n",
       "      <td>868</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>RT @ylecun: Congratulations Jitendra Malik !\\n...</td>\n",
       "      <td>Wed Apr 03 09:03:40 +0000 2019</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>RT @IIITDelhi: Another chance to take admissio...</td>\n",
       "      <td>Wed Apr 03 07:46:02 +0000 2019</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Dear @midasIIITD internship candidates who hav...</td>\n",
       "      <td>Tue Apr 02 04:20:13 +0000 2019</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Looking forward to your paper submission to @I...</td>\n",
       "      <td>Tue Apr 02 02:44:54 +0000 2019</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>RT @ngrams: Reproducibility in multimedia rese...</td>\n",
       "      <td>Tue Apr 02 02:35:44 +0000 2019</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Online application for https://t.co/DJFDrQsHZP...</td>\n",
       "      <td>Mon Apr 01 06:53:08 +0000 2019</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>RT @ACMMM19: A final reminder of the Reproduci...</td>\n",
       "      <td>Sun Mar 31 10:21:24 +0000 2019</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>RT @isarth23: Thanks for the support and help ...</td>\n",
       "      <td>Fri Mar 29 19:43:24 +0000 2019</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Since SemEval-2019 will be held June 6-7, 2019...</td>\n",
       "      <td>Fri Mar 29 17:16:40 +0000 2019</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>+@aggarwal_kartik.\\nCongrats! Wish you many mo...</td>\n",
       "      <td>Fri Mar 29 17:04:30 +0000 2019</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>RT @aggarwal_kartik: Our work (@midasIIITD ) a...</td>\n",
       "      <td>Fri Mar 29 17:03:29 +0000 2019</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Congratulations! @midasIIITD team, @isarth23 @...</td>\n",
       "      <td>Fri Mar 29 17:02:24 +0000 2019</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>@EEMLcommunity @radamihalcea too many deadline...</td>\n",
       "      <td>Fri Mar 29 05:35:22 +0000 2019</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>RT @stanfordnlp: CS224N Natural Language Proce...</td>\n",
       "      <td>Thu Mar 28 16:55:01 +0000 2019</td>\n",
       "      <td>0</td>\n",
       "      <td>716</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>@ACMMM19 is structured around four themes (alp...</td>\n",
       "      <td>Sun Jan 06 04:07:29 +0000 2019</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>ACM Multimedia 2019: First Call for Papers/Cal...</td>\n",
       "      <td>Sun Jan 06 04:05:38 +0000 2019</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>RT @ACMMM19: Happy new year! 2019 started with...</td>\n",
       "      <td>Thu Jan 03 06:14:05 +0000 2019</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>You may follow @midasIIITD updates through fol...</td>\n",
       "      <td>Tue Jan 01 17:21:49 +0000 2019</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>Today, @midasIIITD completed one year at @IIIT...</td>\n",
       "      <td>Tue Jan 01 17:17:56 +0000 2019</td>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>RT @debanjanbhucs: Happy anniversary to @midas...</td>\n",
       "      <td>Tue Jan 01 16:14:24 +0000 2019</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>Two Research Assistant (RA) positions are stil...</td>\n",
       "      <td>Thu Dec 27 10:28:31 +0000 2018</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>Positions for two Research Assistants (RA) are...</td>\n",
       "      <td>Thu Dec 27 09:59:06 +0000 2018</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>RT @TCoolsIT: I wrote a blogpost on CoreNLP hi...</td>\n",
       "      <td>Thu Dec 27 02:33:58 +0000 2018</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>@RatnRajiv delivered a research talk at the Ce...</td>\n",
       "      <td>Wed Dec 26 09:41:04 +0000 2018</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>@midasIIITD look forward to work together. htt...</td>\n",
       "      <td>Tue Dec 25 04:15:15 +0000 2018</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>@the_dhumketu @punnibhai @Hitkul_, FYI. https:...</td>\n",
       "      <td>Tue Dec 25 03:00:21 +0000 2018</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>RT @iiit_hyderabad: KCIS Distinguished Lecture...</td>\n",
       "      <td>Mon Dec 24 07:39:21 +0000 2018</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>RT @StanfordAILab: Our latest blog post is out...</td>\n",
       "      <td>Fri Dec 21 03:50:39 +0000 2018</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>RT @medialab: In two new papers, @PratikShahPh...</td>\n",
       "      <td>Thu Dec 20 18:05:35 +0000 2018</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>RT @RatnRajiv: Wonderful get together with our...</td>\n",
       "      <td>Thu Dec 20 06:14:52 +0000 2018</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>@ACMMM19 is the premier international conferen...</td>\n",
       "      <td>Wed Dec 19 02:16:31 +0000 2018</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>Feel free to contact us if you have any query ...</td>\n",
       "      <td>Tue Dec 18 14:42:27 +0000 2018</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>@NilayShri @RatnRajiv @ACMMM19 Looking forward...</td>\n",
       "      <td>Tue Dec 18 14:27:12 +0000 2018</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>@midasIIITD head, Dr. @RatnRajiv appointed as ...</td>\n",
       "      <td>Tue Dec 18 14:15:32 +0000 2018</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>@ACMMM19 @the_dhumketu fill the volunteer form...</td>\n",
       "      <td>Tue Dec 18 12:34:10 +0000 2018</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>RT @ACMMM19: New in 2019! Volunteer to serve a...</td>\n",
       "      <td>Tue Dec 18 12:33:42 +0000 2018</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>RT @IIITDelhi: Congratulations! authors Yaman ...</td>\n",
       "      <td>Tue Dec 18 04:38:35 +0000 2018</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>Best wishes to @midasIIITD students @_himanshu...</td>\n",
       "      <td>Mon Dec 17 16:57:00 +0000 2018</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>RT @MIT: Computer model could improve human-ma...</td>\n",
       "      <td>Mon Dec 17 15:05:44 +0000 2018</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>RT @kdnuggets: Great @Wired interview with Geo...</td>\n",
       "      <td>Mon Dec 17 14:37:00 +0000 2018</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>RT @harvardmed: A defective metabolic pathway ...</td>\n",
       "      <td>Mon Dec 17 14:05:47 +0000 2018</td>\n",
       "      <td>0</td>\n",
       "      <td>167</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>RT @stanfordnlp: Thread on (neural) reasoning....</td>\n",
       "      <td>Sun Dec 16 15:01:20 +0000 2018</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>Congratulations! Yaman @the_dhumketu, Rohit, S...</td>\n",
       "      <td>Wed Dec 12 03:56:19 +0000 2018</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>Yaman @the_dhumketu presented our idea of reco...</td>\n",
       "      <td>Sat Dec 01 15:24:51 +0000 2018</td>\n",
       "      <td>20</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Text  \\\n",
       "0    Many Congratulations to @midasIIITD student, S...   \n",
       "1    @midasIIITD thanks all students who have appea...   \n",
       "2    @himanchalchandr Meanwhile, complete CV/NLP ta...   \n",
       "3    @sayangdipto123 Submit as per the guideline ag...   \n",
       "4    We request all students whose interview are sc...   \n",
       "5    Other queries: \"none of the Tweeter Apis give ...   \n",
       "6    Other queries: \"do we have to make two differe...   \n",
       "7    Other queries: \"If using Twitter api, it does ...   \n",
       "8    Response to some queries asked by students on ...   \n",
       "9    RT @kdnuggets: Top 8 #Free Must-Read #Books on...   \n",
       "10   @nupur_baghel @PennDATS Congratulation @nupur_...   \n",
       "11   We have emailed the task details to all candid...   \n",
       "12   RT @rfpvjr: Our NAACL paper on polarization in...   \n",
       "13   RT @kdnuggets: Effective Transfer Learning For...   \n",
       "14   RT @stanfordnlp: What’s new in @Stanford CS224...   \n",
       "15   RT @DeepMindAI: Today we're releasing a large-...   \n",
       "16   RT @ylecun: Congratulations Jitendra Malik !\\n...   \n",
       "17   RT @IIITDelhi: Another chance to take admissio...   \n",
       "18   Dear @midasIIITD internship candidates who hav...   \n",
       "19   Looking forward to your paper submission to @I...   \n",
       "20   RT @ngrams: Reproducibility in multimedia rese...   \n",
       "21   Online application for https://t.co/DJFDrQsHZP...   \n",
       "22   RT @ACMMM19: A final reminder of the Reproduci...   \n",
       "23   RT @isarth23: Thanks for the support and help ...   \n",
       "24   Since SemEval-2019 will be held June 6-7, 2019...   \n",
       "25   +@aggarwal_kartik.\\nCongrats! Wish you many mo...   \n",
       "26   RT @aggarwal_kartik: Our work (@midasIIITD ) a...   \n",
       "27   Congratulations! @midasIIITD team, @isarth23 @...   \n",
       "28   @EEMLcommunity @radamihalcea too many deadline...   \n",
       "29   RT @stanfordnlp: CS224N Natural Language Proce...   \n",
       "..                                                 ...   \n",
       "170  @ACMMM19 is structured around four themes (alp...   \n",
       "171  ACM Multimedia 2019: First Call for Papers/Cal...   \n",
       "172  RT @ACMMM19: Happy new year! 2019 started with...   \n",
       "173  You may follow @midasIIITD updates through fol...   \n",
       "174  Today, @midasIIITD completed one year at @IIIT...   \n",
       "175  RT @debanjanbhucs: Happy anniversary to @midas...   \n",
       "176  Two Research Assistant (RA) positions are stil...   \n",
       "177  Positions for two Research Assistants (RA) are...   \n",
       "178  RT @TCoolsIT: I wrote a blogpost on CoreNLP hi...   \n",
       "179  @RatnRajiv delivered a research talk at the Ce...   \n",
       "180  @midasIIITD look forward to work together. htt...   \n",
       "181  @the_dhumketu @punnibhai @Hitkul_, FYI. https:...   \n",
       "182  RT @iiit_hyderabad: KCIS Distinguished Lecture...   \n",
       "183  RT @StanfordAILab: Our latest blog post is out...   \n",
       "184  RT @medialab: In two new papers, @PratikShahPh...   \n",
       "185  RT @RatnRajiv: Wonderful get together with our...   \n",
       "186  @ACMMM19 is the premier international conferen...   \n",
       "187  Feel free to contact us if you have any query ...   \n",
       "188  @NilayShri @RatnRajiv @ACMMM19 Looking forward...   \n",
       "189  @midasIIITD head, Dr. @RatnRajiv appointed as ...   \n",
       "190  @ACMMM19 @the_dhumketu fill the volunteer form...   \n",
       "191  RT @ACMMM19: New in 2019! Volunteer to serve a...   \n",
       "192  RT @IIITDelhi: Congratulations! authors Yaman ...   \n",
       "193  Best wishes to @midasIIITD students @_himanshu...   \n",
       "194  RT @MIT: Computer model could improve human-ma...   \n",
       "195  RT @kdnuggets: Great @Wired interview with Geo...   \n",
       "196  RT @harvardmed: A defective metabolic pathway ...   \n",
       "197  RT @stanfordnlp: Thread on (neural) reasoning....   \n",
       "198  Congratulations! Yaman @the_dhumketu, Rohit, S...   \n",
       "199  Yaman @the_dhumketu presented our idea of reco...   \n",
       "\n",
       "                      Date and Time  Favorite Count  Retweet Count  \\\n",
       "0    Mon Apr 08 07:08:12 +0000 2019              13              2   \n",
       "1    Mon Apr 08 03:27:42 +0000 2019               3              0   \n",
       "2    Sun Apr 07 14:17:29 +0000 2019               0              0   \n",
       "3    Sun Apr 07 14:17:09 +0000 2019               0              0   \n",
       "4    Sun Apr 07 11:43:24 +0000 2019               1              1   \n",
       "5    Sun Apr 07 06:55:19 +0000 2019               5              2   \n",
       "6    Sun Apr 07 06:53:38 +0000 2019               4              1   \n",
       "7    Sun Apr 07 05:32:27 +0000 2019               6              1   \n",
       "8    Sun Apr 07 05:29:40 +0000 2019               7              1   \n",
       "9    Sat Apr 06 17:11:29 +0000 2019               0              2   \n",
       "10   Sat Apr 06 16:43:27 +0000 2019              18              3   \n",
       "11   Fri Apr 05 16:08:37 +0000 2019              11              1   \n",
       "12   Fri Apr 05 04:05:11 +0000 2019               0             16   \n",
       "13   Fri Apr 05 04:04:43 +0000 2019               0             11   \n",
       "14   Wed Apr 03 18:31:53 +0000 2019               0             59   \n",
       "15   Wed Apr 03 17:04:32 +0000 2019               0            868   \n",
       "16   Wed Apr 03 09:03:40 +0000 2019               0             16   \n",
       "17   Wed Apr 03 07:46:02 +0000 2019               0              4   \n",
       "18   Tue Apr 02 04:20:13 +0000 2019               8              1   \n",
       "19   Tue Apr 02 02:44:54 +0000 2019               5              1   \n",
       "20   Tue Apr 02 02:35:44 +0000 2019               0              7   \n",
       "21   Mon Apr 01 06:53:08 +0000 2019               7              2   \n",
       "22   Sun Mar 31 10:21:24 +0000 2019               0             10   \n",
       "23   Fri Mar 29 19:43:24 +0000 2019               0              2   \n",
       "24   Fri Mar 29 17:16:40 +0000 2019               9              1   \n",
       "25   Fri Mar 29 17:04:30 +0000 2019               2              0   \n",
       "26   Fri Mar 29 17:03:29 +0000 2019               0              1   \n",
       "27   Fri Mar 29 17:02:24 +0000 2019               9              1   \n",
       "28   Fri Mar 29 05:35:22 +0000 2019               0              0   \n",
       "29   Thu Mar 28 16:55:01 +0000 2019               0            716   \n",
       "..                              ...             ...            ...   \n",
       "170  Sun Jan 06 04:07:29 +0000 2019               1              0   \n",
       "171  Sun Jan 06 04:05:38 +0000 2019               4              2   \n",
       "172  Thu Jan 03 06:14:05 +0000 2019               0              2   \n",
       "173  Tue Jan 01 17:21:49 +0000 2019               4              2   \n",
       "174  Tue Jan 01 17:17:56 +0000 2019              14              5   \n",
       "175  Tue Jan 01 16:14:24 +0000 2019               0              4   \n",
       "176  Thu Dec 27 10:28:31 +0000 2018               2              1   \n",
       "177  Thu Dec 27 09:59:06 +0000 2018               5              3   \n",
       "178  Thu Dec 27 02:33:58 +0000 2018               0             12   \n",
       "179  Wed Dec 26 09:41:04 +0000 2018              14              2   \n",
       "180  Tue Dec 25 04:15:15 +0000 2018               1              0   \n",
       "181  Tue Dec 25 03:00:21 +0000 2018               2              1   \n",
       "182  Mon Dec 24 07:39:21 +0000 2018               0             11   \n",
       "183  Fri Dec 21 03:50:39 +0000 2018               0             32   \n",
       "184  Thu Dec 20 18:05:35 +0000 2018               0              8   \n",
       "185  Thu Dec 20 06:14:52 +0000 2018               0              3   \n",
       "186  Wed Dec 19 02:16:31 +0000 2018               2              3   \n",
       "187  Tue Dec 18 14:42:27 +0000 2018               3              2   \n",
       "188  Tue Dec 18 14:27:12 +0000 2018               3              0   \n",
       "189  Tue Dec 18 14:15:32 +0000 2018              12              5   \n",
       "190  Tue Dec 18 12:34:10 +0000 2018               1              0   \n",
       "191  Tue Dec 18 12:33:42 +0000 2018               0              7   \n",
       "192  Tue Dec 18 04:38:35 +0000 2018               0              3   \n",
       "193  Mon Dec 17 16:57:00 +0000 2018              14              4   \n",
       "194  Mon Dec 17 15:05:44 +0000 2018               0             20   \n",
       "195  Mon Dec 17 14:37:00 +0000 2018               0             20   \n",
       "196  Mon Dec 17 14:05:47 +0000 2018               0            167   \n",
       "197  Sun Dec 16 15:01:20 +0000 2018               0             10   \n",
       "198  Wed Dec 12 03:56:19 +0000 2018              16              3   \n",
       "199  Sat Dec 01 15:24:51 +0000 2018              20              4   \n",
       "\n",
       "     Images Count  \n",
       "0             NaN  \n",
       "1             NaN  \n",
       "2             NaN  \n",
       "3             NaN  \n",
       "4             NaN  \n",
       "5             NaN  \n",
       "6             NaN  \n",
       "7             NaN  \n",
       "8             NaN  \n",
       "9             NaN  \n",
       "10            NaN  \n",
       "11            NaN  \n",
       "12            NaN  \n",
       "13            1.0  \n",
       "14            NaN  \n",
       "15            NaN  \n",
       "16            NaN  \n",
       "17            NaN  \n",
       "18            NaN  \n",
       "19            NaN  \n",
       "20            NaN  \n",
       "21            NaN  \n",
       "22            NaN  \n",
       "23            NaN  \n",
       "24            NaN  \n",
       "25            NaN  \n",
       "26            NaN  \n",
       "27            NaN  \n",
       "28            NaN  \n",
       "29            NaN  \n",
       "..            ...  \n",
       "170           NaN  \n",
       "171           NaN  \n",
       "172           NaN  \n",
       "173           NaN  \n",
       "174           NaN  \n",
       "175           NaN  \n",
       "176           NaN  \n",
       "177           NaN  \n",
       "178           NaN  \n",
       "179           NaN  \n",
       "180           NaN  \n",
       "181           NaN  \n",
       "182           NaN  \n",
       "183           NaN  \n",
       "184           NaN  \n",
       "185           NaN  \n",
       "186           NaN  \n",
       "187           NaN  \n",
       "188           NaN  \n",
       "189           NaN  \n",
       "190           NaN  \n",
       "191           NaN  \n",
       "192           NaN  \n",
       "193           NaN  \n",
       "194           NaN  \n",
       "195           NaN  \n",
       "196           NaN  \n",
       "197           NaN  \n",
       "198           NaN  \n",
       "199           NaN  \n",
       "\n",
       "[200 rows x 5 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calling above funtion and printing table\n",
    "\n",
    "tweets_filename = 'tweets.json'\n",
    "table = display_tweets(tweets_filename)\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Text', 'Date and Time', 'Favorite Count', 'Retweet Count',\n",
      "       'Images Count'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Columns contained in dataframe\n",
    "print(table.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
